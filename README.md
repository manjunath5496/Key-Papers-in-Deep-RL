<h2>Model-Free RL: </h2>

<p> <strong>Deep Q-Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(1).pdf" style="text-decoration:none;">Playing Atari with Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(2).pdf" style="text-decoration:none;">Deep Recurrent Q-Learning for Partially Observable MDPs</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(3).pdf" style="text-decoration:none;"> Dueling Network Architectures for Deep Reinforcement Learning</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(4).pdf" style="text-decoration:none;">Deep Reinforcement Learning with Double Q-learning</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(5).pdf" style="text-decoration:none;">Prioritized Experience Replay</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(6).pdf" style="text-decoration:none;">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></b></li>    

</ul>
</br>
<p> <strong>Policy Gradients </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(7).pdf" style="text-decoration:none;">Asynchronous Methods for Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(8).pdf" style="text-decoration:none;">Trust Region Policy Optimization</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(9).pdf" style="text-decoration:none;"> High-Dimensional Continuous Control Using Generalized Advantage Estimation</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(10).pdf" style="text-decoration:none;">Emergence of Locomotion Behaviours in Rich Environments</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(11).pdf" style="text-decoration:none;">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(12).pdf" style="text-decoration:none;">Sample Efficient Actor-Critic with Experience Replay</a></b></li> 
 
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(13).pdf" style="text-decoration:none;">Proximal Policy Optimization Algorithms</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(14).pdf" style="text-decoration:none;">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></b></li> 
 
 
 
 
 
 

</ul>

</br>
<p> <strong>Deterministic Policy Gradients </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(15).pdf" style="text-decoration:none;">Deterministic Policy Gradient Algorithms</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(16).pdf" style="text-decoration:none;">Continuous Control With Deep Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(17).pdf" style="text-decoration:none;"> Addressing Function Approximation Error in Actor-Critic Methods</a></b></li>                         
 

</ul>


</br>
<p> <strong>Distributional RL </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(18).pdf" style="text-decoration:none;">A Distributional Perspective on Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(19).pdf" style="text-decoration:none;">Distributional Reinforcement Learning with Quantile Regression</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(20).pdf" style="text-decoration:none;"> Implicit Quantile Networks for Distributional Reinforcement Learning</a></b></li>                         
 
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(21).pdf" style="text-decoration:none;"> Dopamine: A Research Framework for Deep Reinforcement Learning</a></b></li>       

</ul>

</br>
<p> <strong>Policy Gradients with Action-Dependent Baselines </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(22).pdf" style="text-decoration:none;">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(23).pdf" style="text-decoration:none;">Action-depedent Control Variates for Policy Optimization via Stein's Identity</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(24).pdf" style="text-decoration:none;"> The Mirage of Action-Dependent Baselines in Reinforcement Learning</a></b></li>                         
 
    

</ul>
</br>
<p> <strong>Path-Consistency Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(25).pdf" style="text-decoration:none;">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(26).pdf" style="text-decoration:none;">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a></b></li>


</ul>

</br>
<p> <strong>Other Directions for Combining Policy-Learning and Q-Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(27).pdf" style="text-decoration:none;">Combining Policy Gradient and Q-learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(28).pdf" style="text-decoration:none;">The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning</a></b></li>

 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(29).pdf" style="text-decoration:none;">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement LearningBridging the Gap Between Value and Policy Based Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(30).pdf" style="text-decoration:none;">Equivalence Between Policy Gradients and Soft Q-Learning</a></b></li>
    

</ul>

</br>
<p> <strong>Evolutionary Algorithms </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(31).pdf" style="text-decoration:none;">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></b></li>
  
                         
 

</ul>
</br>

<h2>Exploration: </h2>

<p> <strong>Intrinsic Motivation </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(32).pdf" style="text-decoration:none;">VIME: Variational Information Maximizing Exploration</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(33).pdf" style="text-decoration:none;">Unifying Count-Based Exploration and Intrinsic Motivation</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(34).pdf" style="text-decoration:none;"> #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(35).pdf" style="text-decoration:none;">EX<sup>2</sup>: Exploration with Exemplar Models for Deep Reinforcement Learning</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(36).pdf" style="text-decoration:none;">Count-Based Exploration with Neural Density Models</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(37).pdf" style="text-decoration:none;">Curiosity-driven Exploration by Self-supervised Prediction</a></b></li>    
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(38).pdf" style="text-decoration:none;">Large-Scale Study of Curiosity-Driven Learning</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(39).pdf" style="text-decoration:none;">Exploration by Random Network Distillation</a></b></li>    
</ul>
</br>


<p> <strong>Unsupervised RL </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(40).pdf" style="text-decoration:none;">Variational Intrinsic Control</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(41).pdf" style="text-decoration:none;">Diversity is All You Need: Learning Skills without a Reward Function</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(42).pdf" style="text-decoration:none;"> Variational Option Discovery Algorithms</a></b></li>  

</ul>
</br>

<h2>Transfer and Multitask RL: </h2>

<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(43).pdf" style="text-decoration:none;">Progressive Neural Networks</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(44).pdf" style="text-decoration:none;">Reinforcement Learning with Unsupervised Auxiliary Tasks</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(45).pdf" style="text-decoration:none;">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(46).pdf" style="text-decoration:none;">Hindsight Experience Replay</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(47).pdf" style="text-decoration:none;">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(48).pdf" style="text-decoration:none;">Learning an Embedding Space for Transferable Robot Skills</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(49).pdf" style="text-decoration:none;">Universal Value Function Approximators</a></b></li>    
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(50).pdf" style="text-decoration:none;">Mutual Alignment Transfer Learning</a></b></li>

</ul>
</br>


</br>

<h2>Transfer and Multitask RL: </h2>

<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(51).pdf" style="text-decoration:none;">Strategic Attentive Writer for Learning Macro-Actions</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(52).pdf" style="text-decoration:none;">FeUdal Networks for Hierarchical Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(53).pdf" style="text-decoration:none;">Data-Efficient Hierarchical Reinforcement Learning</a></b></li>                         

</ul>
</br>


