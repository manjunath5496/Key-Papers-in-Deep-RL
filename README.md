<h2>Model-Free RL: </h2>

<p> <strong>Deep Q-Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(1).pdf" style="text-decoration:none;">Playing Atari with Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(2).pdf" style="text-decoration:none;">Deep Recurrent Q-Learning for Partially Observable MDPs</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(3).pdf" style="text-decoration:none;"> Dueling Network Architectures for Deep Reinforcement Learning</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(4).pdf" style="text-decoration:none;">Deep Reinforcement Learning with Double Q-learning</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(5).pdf" style="text-decoration:none;">Prioritized Experience Replay</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(6).pdf" style="text-decoration:none;">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></b></li>    

</ul>
</br>
<p> <strong>Policy Gradients </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(7).pdf" style="text-decoration:none;">Asynchronous Methods for Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(8).pdf" style="text-decoration:none;">Trust Region Policy Optimization</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(9).pdf" style="text-decoration:none;"> High-Dimensional Continuous Control Using Generalized Advantage Estimation</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(10).pdf" style="text-decoration:none;">Emergence of Locomotion Behaviours in Rich Environments</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(11).pdf" style="text-decoration:none;">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(12).pdf" style="text-decoration:none;">Sample Efficient Actor-Critic with Experience Replay</a></b></li> 
 
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(13).pdf" style="text-decoration:none;">Proximal Policy Optimization Algorithms</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(14).pdf" style="text-decoration:none;">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></b></li> 
 
 
 
 
 
 

</ul>

</br>
<p> <strong>Deterministic Policy Gradients </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(15).pdf" style="text-decoration:none;">Deterministic Policy Gradient Algorithms</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(16).pdf" style="text-decoration:none;">Continuous Control With Deep Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(17).pdf" style="text-decoration:none;"> Addressing Function Approximation Error in Actor-Critic Methods</a></b></li>                         
 

</ul>


</br>
<p> <strong>Distributional RL </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(18).pdf" style="text-decoration:none;">A Distributional Perspective on Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(19).pdf" style="text-decoration:none;">Distributional Reinforcement Learning with Quantile Regression</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(20).pdf" style="text-decoration:none;"> Implicit Quantile Networks for Distributional Reinforcement Learning</a></b></li>                         
 
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(21).pdf" style="text-decoration:none;"> Dopamine: A Research Framework for Deep Reinforcement Learning</a></b></li>       

</ul>

</br>
<p> <strong>Policy Gradients with Action-Dependent Baselines </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(22).pdf" style="text-decoration:none;">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(23).pdf" style="text-decoration:none;">Action-depedent Control Variates for Policy Optimization via Stein's Identity</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(24).pdf" style="text-decoration:none;"> The Mirage of Action-Dependent Baselines in Reinforcement Learning</a></b></li>                         
 
    

</ul>
</br>
<p> <strong>Path-Consistency Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(25).pdf" style="text-decoration:none;">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(26).pdf" style="text-decoration:none;">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a></b></li>


</ul>

</br>
<p> <strong>Other Directions for Combining Policy-Learning and Q-Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(27).pdf" style="text-decoration:none;">Combining Policy Gradient and Q-learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(28).pdf" style="text-decoration:none;">The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning</a></b></li>

 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(29).pdf" style="text-decoration:none;">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement LearningBridging the Gap Between Value and Policy Based Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(30).pdf" style="text-decoration:none;">Equivalence Between Policy Gradients and Soft Q-Learning</a></b></li>
    

</ul>



