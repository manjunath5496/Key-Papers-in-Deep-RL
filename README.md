<h2>Model-Free RL: </h2>

<p> <strong>Deep Q-Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(1).pdf" style="text-decoration:none;">Playing Atari with Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(2).pdf" style="text-decoration:none;">Deep Recurrent Q-Learning for Partially Observable MDPs</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(3).pdf" style="text-decoration:none;"> Dueling Network Architectures for Deep Reinforcement Learning</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(4).pdf" style="text-decoration:none;">Deep Reinforcement Learning with Double Q-learning</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(5).pdf" style="text-decoration:none;">Prioritized Experience Replay</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(6).pdf" style="text-decoration:none;">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></b></li>    

</ul>
</br>
<p> <strong>Policy Gradients </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(7).pdf" style="text-decoration:none;">Asynchronous Methods for Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(8).pdf" style="text-decoration:none;">Trust Region Policy Optimization</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(9).pdf" style="text-decoration:none;"> High-Dimensional Continuous Control Using Generalized Advantage Estimation</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(10).pdf" style="text-decoration:none;">Emergence of Locomotion Behaviours in Rich Environments</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(11).pdf" style="text-decoration:none;">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(12).pdf" style="text-decoration:none;">Sample Efficient Actor-Critic with Experience Replay</a></b></li> 
 
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(13).pdf" style="text-decoration:none;">Proximal Policy Optimization Algorithms</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(14).pdf" style="text-decoration:none;">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></b></li> 
 
 
 
 
 
 

</ul>

</br>
<p> <strong>Deterministic Policy Gradients </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(15).pdf" style="text-decoration:none;">Deterministic Policy Gradient Algorithms</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(16).pdf" style="text-decoration:none;">Continuous Control With Deep Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(17).pdf" style="text-decoration:none;"> Addressing Function Approximation Error in Actor-Critic Methods</a></b></li>                         
 

</ul>


</br>
<p> <strong>Distributional RL </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(18).pdf" style="text-decoration:none;">A Distributional Perspective on Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(19).pdf" style="text-decoration:none;">Distributional Reinforcement Learning with Quantile Regression</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(20).pdf" style="text-decoration:none;"> Implicit Quantile Networks for Distributional Reinforcement Learning</a></b></li>                         
 
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(21).pdf" style="text-decoration:none;"> Dopamine: A Research Framework for Deep Reinforcement Learning</a></b></li>       

</ul>

</br>
<p> <strong>Policy Gradients with Action-Dependent Baselines </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(22).pdf" style="text-decoration:none;">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(23).pdf" style="text-decoration:none;">Action-depedent Control Variates for Policy Optimization via Stein's Identity</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(24).pdf" style="text-decoration:none;"> The Mirage of Action-Dependent Baselines in Reinforcement Learning</a></b></li>                         
 
    

</ul>
</br>
<p> <strong>Path-Consistency Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(25).pdf" style="text-decoration:none;">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(26).pdf" style="text-decoration:none;">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a></b></li>


</ul>

</br>
<p> <strong>Other Directions for Combining Policy-Learning and Q-Learning </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(27).pdf" style="text-decoration:none;">Combining Policy Gradient and Q-learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(28).pdf" style="text-decoration:none;">The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning</a></b></li>

 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(29).pdf" style="text-decoration:none;">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement LearningBridging the Gap Between Value and Policy Based Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(30).pdf" style="text-decoration:none;">Equivalence Between Policy Gradients and Soft Q-Learning</a></b></li>
    

</ul>

</br>
<p> <strong>Evolutionary Algorithms </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(31).pdf" style="text-decoration:none;">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></b></li>
  
                         
 

</ul>
</br>

<h2>Exploration: </h2>

<p> <strong>Intrinsic Motivation </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(32).pdf" style="text-decoration:none;">VIME: Variational Information Maximizing Exploration</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(33).pdf" style="text-decoration:none;">Unifying Count-Based Exploration and Intrinsic Motivation</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(34).pdf" style="text-decoration:none;"> #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(35).pdf" style="text-decoration:none;">EX<sup>2</sup>: Exploration with Exemplar Models for Deep Reinforcement Learning</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(36).pdf" style="text-decoration:none;">Count-Based Exploration with Neural Density Models</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(37).pdf" style="text-decoration:none;">Curiosity-driven Exploration by Self-supervised Prediction</a></b></li>    
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(38).pdf" style="text-decoration:none;">Large-Scale Study of Curiosity-Driven Learning</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(39).pdf" style="text-decoration:none;">Exploration by Random Network Distillation</a></b></li>    
</ul>
</br>


<p> <strong>Unsupervised RL </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(40).pdf" style="text-decoration:none;">Variational Intrinsic Control</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(41).pdf" style="text-decoration:none;">Diversity is All You Need: Learning Skills without a Reward Function</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(42).pdf" style="text-decoration:none;"> Variational Option Discovery Algorithms</a></b></li>  

</ul>
</br>

<h2>Transfer and Multitask RL: </h2>

<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(43).pdf" style="text-decoration:none;">Progressive Neural Networks</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(44).pdf" style="text-decoration:none;">Reinforcement Learning with Unsupervised Auxiliary Tasks</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(45).pdf" style="text-decoration:none;">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(46).pdf" style="text-decoration:none;">Hindsight Experience Replay</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(47).pdf" style="text-decoration:none;">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(48).pdf" style="text-decoration:none;">Learning an Embedding Space for Transferable Robot Skills</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(49).pdf" style="text-decoration:none;">Universal Value Function Approximators</a></b></li>    
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(50).pdf" style="text-decoration:none;">Mutual Alignment Transfer Learning</a></b></li>

</ul>
</br>


<h2>Hierarchy: </h2>

<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(51).pdf" style="text-decoration:none;">Strategic Attentive Writer for Learning Macro-Actions</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(52).pdf" style="text-decoration:none;">FeUdal Networks for Hierarchical Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(53).pdf" style="text-decoration:none;">Data-Efficient Hierarchical Reinforcement Learning</a></b></li>                         

</ul>
</br>

<h2>Memory: </h2>

<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(54).pdf" style="text-decoration:none;">Model-Free Episodic Control</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(55).pdf" style="text-decoration:none;">Neural Map: Structured Memory for Deep Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(56).pdf" style="text-decoration:none;">Neural Episodic Control</a></b></li>  

  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(57).pdf" style="text-decoration:none;">Unsupervised Predictive Memory in a Goal-Directed Agent</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(58).pdf" style="text-decoration:none;">Relational recurrent neural networks</a></b></li>  


</ul>
</br>

<h2>Model-Based RL: </h2>

<p> <strong>Model is Learned </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(59).pdf" style="text-decoration:none;">Imagination-Augmented Agents for Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(60).pdf" style="text-decoration:none;">Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(61).pdf" style="text-decoration:none;"> Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(62).pdf" style="text-decoration:none;">Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion</a></b></li>  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(63).pdf" style="text-decoration:none;">Recurrent World Models Facilitate Policy Evolution</a></b></li>  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(64).pdf" style="text-decoration:none;">Model-Based Reinforcement Learning via Meta-Policy Optimization</a></b></li>    
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(65).pdf" style="text-decoration:none;">Model-Ensemble Trust-Region Policy Optimization</a></b></li>
</ul>
</br>
<p> <strong>Model is Given </strong> </p>
<hr>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(66).pdf" style="text-decoration:none;">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(67).pdf" style="text-decoration:none;">Thinking Fast and Slow with Deep Learning and Tree Search</a></b></li>


</ul>
</br>
<h2>Meta-RL: </h2>

<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(68).pdf" style="text-decoration:none;">RL<sup>2</sup>: Fast Reinforcement Learning via Slow Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(69).pdf" style="text-decoration:none;">Learning to Reinforcement Learn</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(70).pdf" style="text-decoration:none;"> Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></b></li>                         
  <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(71).pdf" style="text-decoration:none;">A Simple Neural Attentive Meta-Learner</a></b></li>  
   

</ul>


</br>
<h2>Scaling RL: </h2>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(72).pdf" style="text-decoration:none;">Accelerated Methods for Deep Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(73).pdf" style="text-decoration:none;">Distributed Prioritized Experience Replay</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(74).pdf" style="text-decoration:none;">Recurrent Experience Replay in Distributed Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(75).pdf" style="text-decoration:none;">RLlib: Abstractions for Distributed Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(76).pdf" style="text-decoration:none;">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a></b></li>

</ul>

</br>
<h2>RL in the Real World: </h2>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(77).pdf" style="text-decoration:none;">Benchmarking Reinforcement Learning Algorithms on Real-World Robots</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(78).pdf" style="text-decoration:none;">Horizon: Facebook's Open Source Applied Reinforcement Learning Platform</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(79).pdf" style="text-decoration:none;">Learning Dexterous In-Hand Manipulation</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(80).pdf" style="text-decoration:none;">QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation</a></b></li>


</ul>

</br>
<h2>Safety: </h2>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(81).pdf" style="text-decoration:none;">Concrete Problems in AI Safety</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(82).pdf" style="text-decoration:none;">Constrained Policy Optimization</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(83).pdf" style="text-decoration:none;">Deep Reinforcement Learning from Human Preferences</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(84).pdf" style="text-decoration:none;">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(85).pdf" style="text-decoration:none;">Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(86).pdf" style="text-decoration:none;">Safe Exploration in Continuous Action Spaces</a></b></li>

</ul>

</br>
<h2>Imitation Learning and Inverse Reinforcement Learning: </h2>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(87).pdf" style="text-decoration:none;">Generative Adversarial Imitation Learning</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(88).pdf" style="text-decoration:none;">DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skill</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(89).pdf" style="text-decoration:none;">Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(90).pdf" style="text-decoration:none;">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(91).pdf" style="text-decoration:none;">Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(92).pdf" style="text-decoration:none;">One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL</a></b></li>

</ul>

</br>
<h2>Reproducibility, Analysis, and Critique: </h2>
<ul>
  
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(93).pdf" style="text-decoration:none;">Benchmarking Deep Reinforcement Learning for Continuous Control</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(94).pdf" style="text-decoration:none;">Simple random search provides a competitive approach to reinforcement learning</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(95).pdf" style="text-decoration:none;">Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(96).pdf" style="text-decoration:none;">Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?</a></b></li>
 <li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(97).pdf" style="text-decoration:none;">Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</a></b></li>
  
<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(98).pdf" style="text-decoration:none;">Benchmarking Model-Based Reinforcement Learning</a></b></li>

<li><b><a target="_blank" href="https://github.com/manjunath5496/Key-Papers-in-Deep-RL/blob/master/plk(99).pdf" style="text-decoration:none;">Deep Reinforcement Learning that Matters</a></b></li>

</ul>
